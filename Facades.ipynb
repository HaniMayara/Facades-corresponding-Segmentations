{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load the CSV file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  image_id                   domain split     image_path\n",
      "0        1  B (Facade Segmentation)  test    testB/1.jpg\n",
      "1        1  B (Facade Segmentation)  test    testB/1.jpg\n",
      "2       10  B (Facade Segmentation)  test   testB/10.jpg\n",
      "3       10  B (Facade Segmentation)  test   testB/10.jpg\n",
      "4      100  B (Facade Segmentation)  test  testB/100.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "csv_path = \"Dataset/metadata.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Inspect data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Filter and split the dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 800, Test samples: 212\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['split'] == 'train']\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}, Test samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Organize paths***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainA_paths = train_data[train_data['domain'] == 'A (Facade Photo)']['image_path'].tolist()\n",
    "trainB_paths = train_data[train_data['domain'] == 'B (Facade Segmentation)']['image_path'].tolist()\n",
    "\n",
    "testA_paths = test_data[test_data['domain'] == 'A (Facade Photo)']['image_path'].tolist()\n",
    "testB_paths = test_data[test_data['domain'] == 'B (Facade Segmentation)']['image_path'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load and preprocess images***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_images(paths, base_folder, size=(128, 128)):\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        if os.path.isfile(os.path.join(base_folder, path)):  # Ensure the file exists\n",
    "            img = Image.open(os.path.join(base_folder, path)).resize(size)\n",
    "            images.append(np.array(img) / 255.0)  # Normalize\n",
    "    return np.array(images)\n",
    "\n",
    "# Filter paths to include only valid ones for trainA and trainB\n",
    "trainA_paths = train_data[train_data['image_path'].str.contains(\"A.jpg\")]['image_path'].tolist()\n",
    "trainB_paths = train_data[train_data['image_path'].str.contains(\"B.jpg\")]['image_path'].tolist()\n",
    "\n",
    "testA_paths = test_data[test_data['image_path'].str.contains(\"A.jpg\")]['image_path'].tolist()\n",
    "testB_paths = test_data[test_data['image_path'].str.contains(\"B.jpg\")]['image_path'].tolist()\n",
    "\n",
    "# Load train and test images\n",
    "trainA_images = load_images(trainA_paths, base_folder=\"Dataset\")\n",
    "trainB_images = load_images(trainB_paths, base_folder=\"Dataset\")\n",
    "\n",
    "testA_images = load_images(testA_paths, base_folder=\"Dataset\")\n",
    "testB_images = load_images(testB_paths, base_folder=\"Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "# Define the generator (U-Net architecture for RGB output)\n",
    "generator = pix2pix.unet_generator(3, norm_type='batchnorm')\n",
    "\n",
    "# Define the discriminator (PatchGAN)\n",
    "discriminator = pix2pix.discriminator(norm_type='batchnorm', target=False)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "# Loss function\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 128, 128, 3)\n",
      "(400, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trainB_images.shape)\n",
    "print(trainA_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = min(len(trainB_images), len(trainA_images))\n",
    "trainB_images = trainB_images[:min_size]\n",
    "trainA_images = trainA_images[:min_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_test_size = min(len(testB_images), len(testA_images))\n",
    "testB_images = testB_images[:min_test_size]\n",
    "testA_images = testA_images[:min_test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_14592\\3033065970.py\", line 19, in train_step  *\n        gen_output = generator(input_image, training=True)\n    File \"c:\\Users\\pc\\pyver\\py3123\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling Concatenate.call().\n    \n    \u001b[1mDimension 1 in both shapes must be equal, but are 2 and 1. Shapes are [16,2,2] and [16,1,1]. for '{{node functional_148_1/concatenate_6_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](functional_148_1/sequential_116_1/re_lu_42_1/Relu, functional_148_1/sequential_114_1/leaky_re_lu_78_1/LeakyRelu, functional_148_1/concatenate_6_1/concat/axis)' with input shapes: [16,2,2,512], [16,1,1,512], [] and with computed input tensors: input[2] = <-1>.\u001b[0m\n    \n    Arguments received by Concatenate.call():\n      • inputs=['tf.Tensor(shape=(16, 2, 2, 512), dtype=float32)', 'tf.Tensor(shape=(16, 1, 1, 512), dtype=float32)']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_image, target \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[1;32m---> 17\u001b[0m     gen_loss, disc_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Discriminator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisc_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pc\\pyver\\py3123\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefkrkk_rz.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(input_image, target)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m gen_tape, ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m disc_tape:\n\u001b[1;32m---> 11\u001b[0m     gen_output \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     disc_real_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), ([ag__\u001b[38;5;241m.\u001b[39mld(input_image), ag__\u001b[38;5;241m.\u001b[39mld(target)],), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     13\u001b[0m     disc_generated_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), ([ag__\u001b[38;5;241m.\u001b[39mld(input_image), ag__\u001b[38;5;241m.\u001b[39mld(gen_output)],), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n",
      "File \u001b[1;32mc:\\Users\\pc\\pyver\\py3123\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_14592\\3033065970.py\", line 19, in train_step  *\n        gen_output = generator(input_image, training=True)\n    File \"c:\\Users\\pc\\pyver\\py3123\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling Concatenate.call().\n    \n    \u001b[1mDimension 1 in both shapes must be equal, but are 2 and 1. Shapes are [16,2,2] and [16,1,1]. for '{{node functional_148_1/concatenate_6_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](functional_148_1/sequential_116_1/re_lu_42_1/Relu, functional_148_1/sequential_114_1/leaky_re_lu_78_1/LeakyRelu, functional_148_1/concatenate_6_1/concat/axis)' with input shapes: [16,2,2,512], [16,1,1,512], [] and with computed input tensors: input[2] = <-1>.\u001b[0m\n    \n    Arguments received by Concatenate.call():\n      • inputs=['tf.Tensor(shape=(16, 2, 2, 512), dtype=float32)', 'tf.Tensor(shape=(16, 1, 1, 512), dtype=float32)']\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create training and test datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((trainB_images, trainA_images))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((testB_images, testA_images))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for input_image, target in train_dataset:\n",
    "        gen_loss, disc_loss = train_step(input_image, target)\n",
    "    print(f\"Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Generate and visualize results for 3 test images\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     input_image \u001b[38;5;241m=\u001b[39m \u001b[43mtestB_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m generator(input_image, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Plot the input, prediction, and ground truth\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate and visualize results for 3 test images\n",
    "for i in range(3):\n",
    "    input_image = testB_images[i][None, ...]  # Add batch dimension\n",
    "    prediction = generator(input_image, training=False)\n",
    "\n",
    "    # Plot the input, prediction, and ground truth\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input (Sketch)\")\n",
    "    plt.imshow(tf.keras.utils.array_to_img(input_image[0]))\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Generated Image\")\n",
    "    plt.imshow(tf.keras.utils.array_to_img(prediction[0]))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.imshow(tf.keras.utils.array_to_img(testA_images[i]))\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
